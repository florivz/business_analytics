{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8S14s9ADHS9w"
   },
   "source": [
    "# Business Analytics Final Project\n",
    "## by Florian Veitz (IBAIT21) designated for Prof. Dr. Mohebi University of West Florida\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "Also known as Multi-Layer-Perceptrons (MLP). Hence for this assignment you will use the MLPClassifier class from Sklearn.\n",
    "\n",
    "Take a look at the documentation to learn more about the default parameterisation (which activation function it uses, which optimizer/solver it uses, number and size of hidden layers, etc.) of the MLPClassifer:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFFVKTWQJeWd"
   },
   "source": [
    "### Task 1: Neural Network Classifier on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5251,
     "status": "ok",
     "timestamp": 1690164115443,
     "user": {
      "displayName": "Florian Veitz",
      "userId": "00564572511763694047"
     },
     "user_tz": 300
    },
    "id": "GzWI6gk3JdEI"
   },
   "outputs": [],
   "source": [
    "# load required libraries\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM8DvF5fJ5Np"
   },
   "source": [
    "The task will be to perform classification on handwritten digits from 0 to 9 (MNIST dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60967,
     "status": "ok",
     "timestamp": 1690164179706,
     "user": {
      "displayName": "Florian Veitz",
      "userId": "00564572511763694047"
     },
     "user_tz": 300
    },
    "id": "ChOOL3YeJqto",
    "outputId": "2e4d29f4-e05a-4eb1-d762-f9d5a1a0f0e5"
   },
   "outputs": [],
   "source": [
    "# download dataset from https://www.openml.org/ which contains many sample datasets for machine learning\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cuKVOZmJuAM"
   },
   "source": [
    "**Recap**: The dataset contains 70000 examples of which each example has 784 values (pixels). These pixels are in a flat array but represent a 28 by 28 pixel gray-scale image. Values range from 0 to 255 which is common in the RGB value range. A value of 0 represents a black pixel whereas 255 represents a white pixel. Different shades of gray are any value larger than 0 but smaller than 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1690164192871,
     "user": {
      "displayName": "Florian Veitz",
      "userId": "00564572511763694047"
     },
     "user_tz": 300
    },
    "id": "UbIjxFuxO56N",
    "outputId": "74955759-2f33-4e49-f120-81107c960e52"
   },
   "outputs": [],
   "source": [
    "# if we want to plot a single example we need to reshape the array\n",
    "X_numpy = X.to_numpy()\n",
    "def show_image(i):\n",
    "  first_image = np.array(X_numpy[i], dtype='float').reshape((28, 28))\n",
    "  plt.imshow(first_image, cmap='gray')\n",
    "\n",
    "show_image(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6B9x73V-HTQV"
   },
   "source": [
    "#### Introduction\n",
    "\n",
    "**Table of content for ML model:**\n",
    "\n",
    "\n",
    "*   Data preparation:\n",
    " *   Performing a 80/20 train/test split\n",
    " *   Performing feature scaling\n",
    "*   Traininng the model\n",
    " *   Use of the  `MLPClassifier` from `sklearn.neural_network`\n",
    "*   Evaluation of the model performance\n",
    " *   Calculatinng the accuracy\n",
    " *   Ploting the confusion matrix\n",
    " *   Ploting some misclassified instances\n",
    "* Comparition of the model performance with the results of the softmax regression on MNIST in the previous assignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uyx6TfAAQlPi"
   },
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlj7nJ_HQuFp"
   },
   "source": [
    "Perform a 80/20 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 642,
     "status": "ok",
     "timestamp": 1690164478108,
     "user": {
      "displayName": "Florian Veitz",
      "userId": "00564572511763694047"
     },
     "user_tz": 300
    },
    "id": "qD_mQ_OPLFmP"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "train_data_x, test_data_x, train_data_y, test_data_y = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUO5g9jwQ9Sh"
   },
   "source": [
    "Perform feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1690164480774,
     "user": {
      "displayName": "Florian Veitz",
      "userId": "00564572511763694047"
     },
     "user_tz": 300
    },
    "id": "DdFSHAlcLFbu"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_train_data_x = train_data_x.copy()\n",
    "scaled_test_data_x = test_data_x.copy()\n",
    "\n",
    "scaled_train_data_x = scaler.fit_transform(train_data_x)\n",
    "scaled_test_data_x = scaler.transform(test_data_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_jRHX-HRDPx"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "executionInfo": {
     "elapsed": 5353,
     "status": "ok",
     "timestamp": 1690164487861,
     "user": {
      "displayName": "Florian Veitz",
      "userId": "00564572511763694047"
     },
     "user_tz": 300
    },
    "id": "vMuxvInJLFTZ",
    "outputId": "a5a01604-581c-4af4-b1d0-6e9a8f49f0b5"
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier()\n",
    "# limiting the training set in order to keep the training time low\n",
    "size = 3000\n",
    "train_data_x = train_data_x.values[:size].reshape(-1, len(X.columns))\n",
    "train_data_y = train_data_y.values[:size].reshape(-1)\n",
    "test_data_x = test_data_x.values.reshape(-1, len(X.columns))\n",
    "test_data_y = test_data_y.values.reshape(-1)\n",
    "\n",
    "mlp.fit(train_data_x, train_data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtxdOCTFRrL6"
   },
   "source": [
    "### Evaluation of the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6wDdtVLRu35"
   },
   "source": [
    "Calculating the accuracy and ploting the confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "executionInfo": {
     "elapsed": 1676,
     "status": "ok",
     "timestamp": 1690164540990,
     "user": {
      "displayName": "Florian Veitz",
      "userId": "00564572511763694047"
     },
     "user_tz": 300
    },
    "id": "5_jCp-V-RyIK",
    "outputId": "58b81cc8-6523-494c-f350-9eecf4a8763b"
   },
   "outputs": [],
   "source": [
    "predictions = mlp.predict(test_data_x)\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"Accuracy: {accuracy_score(test_data_y, predictions)}\")\n",
    "\n",
    "labels = range(0, 10)\n",
    "\n",
    "conf_matrix = confusion_matrix(test_data_y, predictions)\n",
    "cm_display = ConfusionMatrixDisplay(conf_matrix, display_labels=labels)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3460,
     "status": "ok",
     "timestamp": 1690164556116,
     "user": {
      "displayName": "Florian Veitz",
      "userId": "00564572511763694047"
     },
     "user_tz": 300
    },
    "id": "WrdeYL3xSQ9J",
    "outputId": "05077309-2866-4270-fecb-db944bb9679f"
   },
   "outputs": [],
   "source": [
    "count_miss = 0\n",
    "\n",
    "for i, y in enumerate(test_data_y):\n",
    "    pred_y = predictions[i]\n",
    "\n",
    "    if (pred_y != y):\n",
    "\n",
    "        count_miss = count_miss + 1\n",
    "        print(f\"Predicted value: {pred_y}. Actual value: {y}\" )\n",
    "        show_image(i)\n",
    "        plt.show()\n",
    "\n",
    "        if count_miss is 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ODadjEdHTVE"
   },
   "source": [
    "### Task 2: Neural Network Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1690164989049,
     "user": {
      "displayName": "Florian Veitz",
      "userId": "00564572511763694047"
     },
     "user_tz": 300
    },
    "id": "7Aoa13hkLTvC"
   },
   "outputs": [],
   "source": [
    "# load required libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from pandas.io.stata import relativedelta\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pll7ms5wYPHg"
   },
   "source": [
    "#### Dataset\n",
    "\n",
    "This is a classic marketing bank dataset uploaded originally in the UCI Machine Learning Repository and contains >41k records. You can find more information about the features (attributes) on the official UCI website:\n",
    "https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "\n",
    "The dataset gives you information about a marketing campaign of a financial institution in which can be analysed in order to find ways to look for future strategies in order to improve future marketing campaigns for the bank.\n",
    "\n",
    "The target variable is called 'deposit' which describes if a person has subscribed to a term deposit (more information: https://www.investopedia.com/terms/t/termdeposit.asp).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 373,
     "status": "error",
     "timestamp": 1690164992290,
     "user": {
      "displayName": "Florian Veitz",
      "userId": "00564572511763694047"
     },
     "user_tz": 300
    },
    "id": "rFlMKv4VLUCL",
    "outputId": "be642f33-87e9-4639-d0e1-a5c1a38a235c"
   },
   "outputs": [],
   "source": [
    "# Import the data\n",
    "df = pd.read_csv('')\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFT2cB4FUFdq"
   },
   "source": [
    "#### Instructions\n",
    "\n",
    "This task will combine a lot of different aspect of what we have discussed in class over the past weeks.\n",
    "\n",
    "**You are expected to do:**\n",
    "\n",
    "*   **Data exploration** (6 points):\n",
    " *   Check which features are available.\n",
    " > *   Can some features directly be discarded?\n",
    " *   Check if data is messy (e.g. missing values)\n",
    " *   Check for correlation with target variable\n",
    " *   Look for outliers\n",
    " *   Class distribution\n",
    "*   **Data preparation** (6 points):\n",
    " *   Perform some data cleaning e.g.\n",
    "  >  *   Replace missing values\n",
    "  >  *   Outlier handling\n",
    "  >  *   Removal of duplicates\n",
    " *   Convert non-numeric features to numeric features\n",
    " *   Perform a 80/20 train/test split\n",
    " *   Perform feature scaling\n",
    " *   In case of class imbalance, think about how you want to deal with it. Please briefly explain your decision.\n",
    "*   **Training and model evaluation** (10 points):\n",
    " *   Please use `MLPClassifier` from `sklearn.neural_network`\n",
    " > *   The model should have 4 hidden layers with sizes hidden_layer_size=(10, 1) (parameter hidden_layer_sizes)\n",
    " > *   Set the batch_size to 64\n",
    " *   Evaluate the model performance\n",
    " > *   Calculate the accuracy and other metrics which might be helpful to evaluate the model's performance\n",
    " > *   Based on you findings, describe some measures you could take to improve the model's performance even further.\n",
    " > *   Try to analyse if you see indications of underfitting or overfitting and which countermeasures you could take.\n",
    " *   Please train another model using one of the techniques we have discussed in the lectures and compare the performance to the performance achieved with the neural network.\n",
    "\n",
    "\n",
    "**For each decision you make, briefly explain your reasoning.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDxM36EHW0tB"
   },
   "source": [
    "###Data exploration (6 points):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdtc72RLXBFY"
   },
   "source": [
    "\n",
    "*   Check which features are available.\n",
    "  *  Can some features directly be discarded?\n",
    "*   Check if data is messy (e.g. missing values)\n",
    "*   Check for correlation with target variable\n",
    "*   Look for outliers\n",
    "*   Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dgao6g05XJmg",
    "outputId": "f5e8acc0-5cb2-4288-bfdc-fac28a643519"
   },
   "outputs": [],
   "source": [
    "#available features\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v4Bt5Cyodru"
   },
   "source": [
    "As mentioned in the description of the data set, the feature duration should not be used for a realistic predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0ttqxo_oowQQ",
    "outputId": "9b225371-98ce-4b6d-92ee-e353223bc16b"
   },
   "outputs": [],
   "source": [
    "df = df.drop(['duration'], axis=1)\n",
    "X = X.drop(['duration'], axis=1)\n",
    "df.head()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XOWBqNHdbTQw",
    "outputId": "c34d7fcb-2518-4512-db31-bbc8887e84ec"
   },
   "outputs": [],
   "source": [
    "#class distribution\n",
    "for name, series in X.items():\n",
    "  plot = sns.histplot(data=X, x=name)\n",
    "  plt.xticks(rotation=60)\n",
    "  plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r2LhWDxbd6MR",
    "outputId": "ca643cd9-9431-4676-f607-621adb30faa9"
   },
   "outputs": [],
   "source": [
    "#closer look at specific values of some features\n",
    "print(f\"Length of dataset: {len(df)}\")\n",
    "print(df['job'].value_counts()['unknown'], 'x unknown at job')\n",
    "print(df['education'].value_counts()['unknown'], 'x unknown at education')\n",
    "print(df['contact'].value_counts()['unknown'], 'x unknown at contact')\n",
    "print(df['poutcome'].value_counts()['unknown'], 'x unknown at poutcome')\n",
    "# print(df['pdays'].value_counts()[-1], 'x -1 days contacted')\n",
    "# print(df['previous'].value_counts()[0], 'x not prev contacted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xfFcn6Riuso"
   },
   "source": [
    "The dataset contains some features where the values are unknown. As the percentage of unknown values in the contact and especially the poutcome column is rather high, one cannot delete these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QbhxsWehczrQ",
    "outputId": "ed026bb9-6c96-4660-c62e-706429b77764"
   },
   "outputs": [],
   "source": [
    "#label encoding for non-numeric columns\n",
    "le = LabelEncoder()\n",
    "\n",
    "# get all str columns\n",
    "str_columns = df.select_dtypes(include=\"object\")\n",
    "# encode all str columns\n",
    "for c in str_columns:\n",
    "  df[c] = le.fit_transform(df[c])\n",
    "\n",
    "#first rows of encoded data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4o7YI8wmcoq_",
    "outputId": "d4ddeef3-bfb1-4bef-937a-05ed2ca35c5d"
   },
   "outputs": [],
   "source": [
    "#For getting an overview of how much influece which single feature has on the income\n",
    "#Kernel density estimations (KDE) of each feature\n",
    "for c in df.columns:\n",
    "  sns.displot(df, x=c, hue='deposit', kind='kde', fill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4pYONjVFetqI",
    "outputId": "9e5b2d62-e8f4-45bf-cd10-61b9d95cbdf7"
   },
   "outputs": [],
   "source": [
    "#correlation\n",
    "#last column is relevant for our use case\n",
    "print(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNdXBcaneZkV"
   },
   "source": [
    "By taking a closer look at the features' distribution, KDE and correlation, one can see that there is no feature which does not seem to have any influence on the deposit. Of course, some features have a larger influece on the outcome than others, but from our point of view one should not directly discard any feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JNVDPebXJ_uN",
    "outputId": "f47eddf0-50d1-48aa-a785-1c130ad88517"
   },
   "outputs": [],
   "source": [
    "for name, series in df.items():\n",
    "  print(name)\n",
    "  plot = plt.boxplot(df[name])\n",
    "  plt.xticks(rotation=60)\n",
    "  print(plt.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGqo9JdzIi0Y"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyax3OKqdDgE"
   },
   "source": [
    " ### Data preparation (6 points):\n",
    " *   Perform some data cleaning e.g.\n",
    "  >  *   Replace missing values\n",
    "  >  *   Outlier handling\n",
    "  >  *   Removal of duplicates\n",
    " *   Convert non-numeric features to numeric features\n",
    " *   Perform a 80/20 train/test split\n",
    " *   Perform feature scaling\n",
    " *   In case of class imbalance, think about how you want to deal with it. Please briefly explain your decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRVXT9qVdPvR"
   },
   "source": [
    "####Data cleaning\n",
    "* We cannot replace missing values ('unknown') with useful, for example median, data as we only have unkown values in nominal features. We decided to let 'unknown' be a separate category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TFN0Vz7yd9QH",
    "outputId": "4a701ef3-60e1-4d0f-a7a5-5e6ce0c428ee"
   },
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df_clean = df.drop_duplicates()\n",
    "print(len(df_clean))\n",
    "df = df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oD9ebImU42Ag"
   },
   "source": [
    "* There was one duplicates which was removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5zl1OgKLmPV"
   },
   "source": [
    "We cut outliers by means of the following outlier detection function which is based on the z-standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhamiX3MK7j-"
   },
   "outputs": [],
   "source": [
    "def remove_outlier(df, columns_to_remove, z_threshold):\n",
    "  outliers = []\n",
    "  data_1 = df.copy()\n",
    "\n",
    "  for column in columns_to_remove:\n",
    "    # calculate mean and std of every column\n",
    "    mean_1 = np.mean(df[column])\n",
    "    std_1 = np.std(df[column])\n",
    "\n",
    "    # z_score = (y - mean_1) / std_1\n",
    "    # =>\n",
    "    y_max = z_threshold * std_1 + mean_1\n",
    "    y_min = (-z_threshold) * std_1 + mean_1\n",
    "\n",
    "    print(column)\n",
    "    print(f\"Max: {y_max}\")\n",
    "    print(f\"Min: {y_min}\")\n",
    "\n",
    "    data_1 = data_1[ (data_1[column] > y_min ) & (data_1[column] < y_max) ]\n",
    "\n",
    "  return data_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t37dlVt421sV"
   },
   "source": [
    "We decided to only focus on three columns for outlier detection as they contain numeric values and clear outliers can be seen in the boxplots above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I2rSPis_TGXN",
    "outputId": "d5484b4f-e7f7-4f72-e1b7-de916caa2a27"
   },
   "outputs": [],
   "source": [
    "outlier_columns = ['age', 'balance', 'campaign']\n",
    "df_filtered = remove_outlier(df, outlier_columns, 3)\n",
    "\n",
    "print(len(df.drop_duplicates()))\n",
    "print(len(df_filtered))\n",
    "\n",
    "print(df_filtered.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwFAaHqGeTyG"
   },
   "source": [
    "---\n",
    "Non-numeric data have already been converted by encoding in the data exploration part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-aCtRu0gi9JA",
    "outputId": "9c1d277a-ce6a-491b-e43f-626773493ee8"
   },
   "outputs": [],
   "source": [
    "#New X and y due to outlier removal\n",
    "X = df_filtered.iloc[:, :-1]\n",
    "y = df_filtered.iloc[:, -1]\n",
    "#checking balance of the dataset\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b61huA9PjABR"
   },
   "source": [
    "The dataset is not balanced. To counter this we decided to perform a stratified train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPoY9Re_emjR"
   },
   "outputs": [],
   "source": [
    "#train test split\n",
    "np.random.seed(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8, test_size = 0.2, random_state = 100, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CB_8EBoJjMdQ",
    "outputId": "2bcc7021-58bf-451a-e4eb-50dbc8ec18f7"
   },
   "outputs": [],
   "source": [
    "#checking balance\n",
    "print('Training data')\n",
    "print(y_train.value_counts())\n",
    "print('Test data')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qj_tlYkCkte-"
   },
   "source": [
    "Now we ensured that the relation between the positive and negative outcome is the same in the train and the test data set as in the overall data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5SqnwGP5fQD5",
    "outputId": "3fb9487b-caf7-460e-875b-6ffea81ded6c"
   },
   "outputs": [],
   "source": [
    "#feature scaling\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train[:] = scaler.fit_transform(X_train)\n",
    "X_test[:] = scaler.transform(X_test)\n",
    "\n",
    "print(X_train.head())\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oirq9uom2Js"
   },
   "source": [
    "## Training and model evaluation (10 points):\n",
    "*   **Training and model evaluation** (10 points):\n",
    " *   Please use `MLPClassifier` from `sklearn.neural_network`\n",
    " > *   The model should have 4 hidden layers with sizes hidden_layer_size=(10, 1) (parameter hidden_layer_sizes)\n",
    " > *   Set the batch_size to 64\n",
    " *   Evaluate the model performance\n",
    " > *   Calculate the accuracy and other metrics which might be helpful to evaluate the model's performance\n",
    " > *   Based on you findings, describe some measures you could take to improve the model's performance even further.\n",
    " > *   Try to analyse if you see indications of underfitting or overfitting and which countermeasures you could take.\n",
    " *   Please train another model using one of the techniques we have discussed in the lectures and compare the performance to the performance achieved with the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "520TPG2-_wVz",
    "outputId": "5298594c-924b-42ec-cde1-edf5b51c6e35"
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10, 10) , batch_size = 64, max_iter = 500)\n",
    "X_train = X_train.values.reshape(-1, len(X.columns))\n",
    "y_train = y_train.values.reshape(-1)\n",
    "X_test = X_test.values.reshape(-1, len(X.columns))\n",
    "y_test = y_test.values.reshape(-1)\n",
    "\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "kWB22JVOIeM_",
    "outputId": "fff4fc46-1ef7-48a1-c303-51152e4f8aea"
   },
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "print(f\"Precision: {precision_score(y_test, predictions)}\")\n",
    "print(f\"Recall: {recall_score(y_test, predictions)}\")\n",
    "\n",
    "labels = range(0, 2)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predictions, normalize='true')\n",
    "cm_display = ConfusionMatrixDisplay(conf_matrix, display_labels=labels)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0zx8QxBJ0fb"
   },
   "source": [
    "The model has an accuracy of about 68%, precision of ~70% and recall of ~59%. 59% are true-positive predictions, 77% true-negative. The false-positive rate is 23% and false-negative rate is 41%. Our dummy prediction would be to always predict the majority class - here it is the value 0. This would achieve an accuracy of 1121/(1121+1011) ~ 52,53% in our test data set. In comparision, our model has a moderate performance, but still is significantly better than our baseline accuracy.\n",
    "\n",
    "In our opinion, the precision is the most relevant metric with our use-case.\n",
    "It describes how many of the positively predicted labels are actually true, so how many of the predicted customers doing deposits did actually deposit money after calls.\n",
    "\n",
    "There are several ways to improve the model's performance:\n",
    "- On the one hand, one could collect more data in general which is then used for a new larger training set.\n",
    "- On the other hand, one could think of another distribution of the size of our training and test set.\n",
    "- Another option would be to handle outliers differently. For example by cutting more our less outlieres (change of z-threshold) or change the features of which outliers rows are cut.\n",
    "- Furthermore, not to use all features but to cut the features with the fewest correlation might lead to an improvement. But an decrease in features might also lead to underfitting.\n",
    "- We can also try out another activation function and optimization function.\n",
    "- We could adapt the model by changing the number of hidden layers and nodes of each layer.\n",
    "- Decrease the learning rate of the model for a finer/better fit. We could also use different modes of the learning rate, for example an adaptive rate that decreases over time. This can lead to a better fit with the same training time used.\n",
    "- Decrease the batch size which makes the model training slower, but can achieve a better result.\n",
    "- Increase the training epochs to achieve a better model fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXxQsFF_2Ly8"
   },
   "source": [
    "**Over-/Underfitting:**\n",
    "\n",
    "There are several features that have only a very small correlation with our dependent variable y \"deposit\" (e.g. corr(age, deposit) = 0,0349).\n",
    "We could cut some of these features and train the model again. If the model then has a better accuracy it was probably overfitted before.\n",
    "We can also compare the training accuracy to the test accuracy. A higher training accuracy than the test accuracy can be a sign of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTd2lt22hYus",
    "outputId": "624afa0e-8ff8-4934-9fe1-24df72f7f670"
   },
   "outputs": [],
   "source": [
    "# Detect overfitting:\n",
    "## OF when training_acc is much greater than test_acc\n",
    "\n",
    "\n",
    "train_pred = mlp.predict(X_train)\n",
    "\n",
    "print(f\"Train Accuracy: {accuracy_score(y_train, train_pred)}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2FELiCb2SX5"
   },
   "source": [
    "Our model's training accuracy is ~3% better than its test accuracy. This indicates a slight overfitting. To counter this we can use the following methods:\n",
    "\n",
    "* Cut some features (with a low correlation to y) and see if the model performs better.\n",
    "* Increase the amount of regularization used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk9Re-WGuK3O"
   },
   "source": [
    "###Another Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktFuAiASjuAB"
   },
   "source": [
    "Logistic Regression Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "msASIW7Niszu",
    "outputId": "8704779a-6dbb-40c0-878c-d3a1f334ec7f"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "log_pred = logreg.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, log_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, log_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, log_pred)}\")\n",
    "\n",
    "labels = range(0, 2)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, log_pred, normalize='true')\n",
    "cm_display = ConfusionMatrixDisplay(conf_matrix, display_labels=labels)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Gi_-EnVjw9P"
   },
   "source": [
    "KNN Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "GvGkKV41jy52",
    "outputId": "faf26b19-6cc9-47ae-f2ee-d6567f525ef4"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "knn_pred = knn.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, knn_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, knn_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, knn_pred)}\")\n",
    "\n",
    "labels = range(0, 2)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, knn_pred, normalize='true')\n",
    "cm_display = ConfusionMatrixDisplay(conf_matrix, display_labels=labels)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSPsXK9jkEjn"
   },
   "source": [
    "Decision Tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "xa36vPM7kG3Y",
    "outputId": "a8d041af-23ea-4e4d-d995-e8d37fe70187"
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "dt_pred = dt.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, dt_pred)}\")\n",
    "print(f\"Precision: {precision_score(y_test, dt_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, dt_pred)}\")\n",
    "\n",
    "labels = range(0, 2)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, dt_pred, normalize='true')\n",
    "cm_display = ConfusionMatrixDisplay(conf_matrix, display_labels=labels)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbPWfJaln4tg"
   },
   "source": [
    "We trained 3 different models for comparison, where the best model of these was the LogisticRegression with these metrics:\n",
    "\n",
    "Accuracy: 0.667\n",
    "\n",
    "Precision: 0.647\n",
    "\n",
    "Recall: 0.654\n",
    "\n",
    "Our Neural Network has these metrics:\n",
    "\n",
    "Accuracy: 0.683\n",
    "\n",
    "Precision: 0.700\n",
    "\n",
    "Recall: 0.589\n",
    "\n",
    "So our model has a better accuracy (+1,6%) and even better precision (+5,3%), but a worse recall (-6,5%).\n",
    "\n",
    "We would still use the Neural Network model, as for us it would be the most important that we have the best succession rate for our selected calls (determined by the precision).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ni-dia8guTcg",
    "outputId": "11b7d729-2e6c-47d5-c3cc-bc985887bf89"
   },
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10, 10) , batch_size = 64, activation = 'tanh', max_iter = 500)\n",
    "\n",
    "mlp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "KHt-EHCQuUT9",
    "outputId": "52e12022-397d-42a9-ab53-2adc0d41afba"
   },
   "outputs": [],
   "source": [
    "predictions = mlp.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, predictions)}\")\n",
    "\n",
    "labels = range(0, 2)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, predictions, normalize='true')\n",
    "cm_display = ConfusionMatrixDisplay(conf_matrix, display_labels=labels)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJ3TtWzSp3bv"
   },
   "source": [
    "# Sandbox Hyperparameter Tuning\n",
    "\n",
    "We did some hyperparameter tuning for our neural network.\n",
    "This needs VEEEEERYYY long for computing, so we have documented our results here.\n",
    "\n",
    "We had to realize that our hyperparameters below do not improve our model.\n",
    "\n",
    "'Best' hyperparameters:\n",
    "\n",
    "{'activation': 'identity', 'alpha': 0.05, 'batch_size': 20, 'hidden_layer_sizes': (20, 20, 20, 20), 'learning_rate': 'constant', 'solver': 'lbfgs'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IUt2y2kw2Nl"
   },
   "outputs": [],
   "source": [
    "# mlp_gs = MLPClassifier(max_iter=500)\n",
    "# parameter_space = {\n",
    "#     'hidden_layer_sizes': [(10,10,10,10),(5,5,5,5),(20,20,20,20), (10,10,10), (10,10)],\n",
    "#     'activation': ['tanh', 'relu', 'identity', 'logistic'],\n",
    "#     'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "#     'alpha': [0.0001, 0.05],\n",
    "#     'learning_rate': ['constant','adaptive', 'invscaling'],\n",
    "#     'batch_size': [64, 20, 100],\n",
    "# }\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\n",
    "# clf.fit(X, y) # X is train samples and y is the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "Z510OFf_4naJ",
    "outputId": "91c9ecf8-08d8-4fac-f310-ab92c372d8fb"
   },
   "outputs": [],
   "source": [
    "mlp2 = MLPClassifier(activation='identity', alpha=0.05, batch_size=20, hidden_layer_sizes= (20, 20, 20, 20), learning_rate='constant', solver='lbfgs', max_iter = 500)\n",
    "X_train = X_train.reshape(-1, len(X.columns))\n",
    "y_train = y_train.reshape(-1)\n",
    "X_test = X_test.reshape(-1, len(X.columns))\n",
    "y_test = y_test.reshape(-1)\n",
    "\n",
    "mlp2.fit(X_train, y_train)\n",
    "\n",
    "pred2 = mlp2.predict(X_test)\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"Accuracy: {accuracy_score(y_test, pred2)}\")\n",
    "print(f\"Precision: {precision_score(y_test, pred2)}\")\n",
    "print(f\"Recall: {recall_score(y_test, pred2)}\")\n",
    "\n",
    "labels = range(0, 2)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, pred2, normalize='true')\n",
    "cm_display = ConfusionMatrixDisplay(conf_matrix, display_labels=labels)\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBJCQGjRlt1X"
   },
   "source": [
    "\n",
    "#### Further tips for working on the assignment:\n",
    "\n",
    "When analyzing the model's performance, please think about what the baseline performance of the task would be and if your model performs better or not. It is quite unlikely the model will get a perfect score with the given parametrization. You can try to improve the performance by varying several hyperparameters of the model (e.g. number of hidden layers and number of neurons in a hidden layer, batch_size, training epochs, etc.).\n",
    "\n",
    "Please be aware that too many hidden layers and neurons and a large number of epochs will cause the model to train longer. If the model is too complex you might encounter time-outs in Colab.\n",
    "\n",
    "If the number of epochs is too low, sklearn will show a warning that the model has not yet converged."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/N4ims/atit2_ass2/blob/main/assignments/Assignment3_ATIT22.ipynb",
     "timestamp": 1670772551382
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
